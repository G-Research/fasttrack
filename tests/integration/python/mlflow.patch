diff --git a/tests/tracking/integration_test_utils.py b/tests/tracking/integration_test_utils.py
index 41ec85f..a99de1d 100644
--- a/tests/tracking/integration_test_utils.py
+++ b/tests/tracking/integration_test_utils.py
@@ -52,14 +52,16 @@ def _init_server(backend_uri, root_artifact_uri):
     server_port = get_safe_port()
     process = Popen(
         [
-            sys.executable,
-            "-c",
-            f'from mlflow.server import app; app.run("{LOCALHOST}", {server_port})',
+            "fml",
+            "server",
         ],
         env={
             **os.environ,
-            BACKEND_STORE_URI_ENV_VAR: backend_uri,
-            ARTIFACT_ROOT_ENV_VAR: root_artifact_uri,
+            "FML_LISTEN_ADDRESS": f"{LOCALHOST}:{server_port}",
+            "FML_DATABASE_URI": backend_uri,
+            "FML_ARTIFACT_ROOT": root_artifact_uri,
+            "FML_LOG_LEVEL": "debug",
+            "FML_DATABASE_RESET": str(backend_uri.startswith("postgres://")),
         },
     )
 
diff --git a/tests/tracking/test_rest_tracking.py b/tests/tracking/test_rest_tracking.py
index 45ee41e..3afdfcb 100644
--- a/tests/tracking/test_rest_tracking.py
+++ b/tests/tracking/test_rest_tracking.py
@@ -51,7 +51,7 @@ from tests.tracking.integration_test_utils import (
 _logger = logging.getLogger(__name__)
 
 
-@pytest.fixture(params=["file", "sqlalchemy"])
+@pytest.fixture(params=["sqlite+memory", "sqlite+file", "sqlite+key", "postgres"])
 def mlflow_client(request, tmp_path):
     """Provides an MLflow Tracking API client pointed at the local tracking server."""
     if request.param == "file":
@@ -61,6 +61,14 @@ def mlflow_client(request, tmp_path):
         backend_uri = ("sqlite://" if sys.platform == "win32" else "sqlite:////") + path[
             len("file://") :
         ]
+    elif request.param == "sqlite+memory":
+        backend_uri = f"sqlite://{tmp_path.joinpath('sqlalchemy.db')}?mode=memory&cache=shared"
+    elif request.param == "sqlite+file":
+        backend_uri = f"sqlite://{tmp_path.joinpath('sqlalchemy.db')}?_journal_mode=WAL"
+    elif request.param == "sqlite+key":
+        backend_uri = f"sqlite://{tmp_path.joinpath('sqlalchemy.db')}?_key=passphrase&_journal_mode=WAL"
+    elif request.param == "postgres":
+        backend_uri = "postgres://postgres:postgres@postgres/postgres"
 
     url, process = _init_server(backend_uri, root_artifact_uri=tmp_path.as_uri())
     yield MlflowClient(url)
@@ -140,7 +148,7 @@ def test_create_experiment_validation(mlflow_client):
         {
             "name": 123,
         },
-        "Invalid value 123 for parameter 'name'",
+        "Invalid value for parameter 'name'",
     )
     assert_bad_request({}, "Missing value for required parameter 'name'")
     assert_bad_request(
@@ -149,7 +157,7 @@ def test_create_experiment_validation(mlflow_client):
             "artifact_location": 9.0,
             "tags": [{"key": "key", "value": "value"}],
         },
-        "Invalid value 9.0 for parameter 'artifact_location'",
+        "Invalid value for parameter 'artifact_location'",
     )
     assert_bad_request(
         {
@@ -157,7 +165,7 @@ def test_create_experiment_validation(mlflow_client):
             "artifact_location": "my_location",
             "tags": "5",
         },
-        "Invalid value 5 for parameter 'tags'",
+        "Invalid value for parameter 'tags'",
     )
 
 
@@ -324,7 +332,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": 59,
             "step": 26,
         },
-        "Invalid value 31 for parameter 'run_id' supplied",
+        "Invalid value for parameter 'run_id' supplied",
     )
     assert_bad_request(
         {
@@ -334,7 +342,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": 59,
             "step": 26,
         },
-        "Invalid value 31 for parameter 'key' supplied",
+        "Invalid value for parameter 'key' supplied",
     )
     assert_bad_request(
         {
@@ -344,7 +352,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": 59,
             "step": "foo",
         },
-        "Invalid value foo for parameter 'step' supplied",
+        "Invalid value for parameter 'step' supplied",
     )
     assert_bad_request(
         {
@@ -354,7 +362,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": "foo",
             "step": 41,
         },
-        "Invalid value foo for parameter 'timestamp' supplied",
+        "Invalid value for parameter 'timestamp' supplied",
     )
     assert_bad_request(
         {
@@ -408,7 +416,7 @@ def test_log_param_validation(mlflow_client):
             "key": "param",
             "value": 41,
         },
-        "Invalid value 31 for parameter 'run_id' supplied",
+        "Invalid value for parameter 'run_id' supplied",
     )
     assert_bad_request(
         {
@@ -416,7 +424,7 @@ def test_log_param_validation(mlflow_client):
             "key": 31,
             "value": 41,
         },
-        "Invalid value 31 for parameter 'key' supplied",
+        "Invalid value for parameter 'key' supplied",
     )
 
 
@@ -478,7 +486,7 @@ def test_set_tag_validation(mlflow_client):
             "key": "tag",
             "value": 41,
         },
-        "Invalid value 31 for parameter 'run_id' supplied",
+        "Invalid value for parameter 'run_id' supplied",
     )
     assert_bad_request(
         {
@@ -486,7 +494,7 @@ def test_set_tag_validation(mlflow_client):
             "key": "param",
             "value": 41,
         },
-        "Invalid value 41 for parameter 'value' supplied",
+        "Invalid value for parameter 'value' supplied",
     )
     assert_bad_request(
         {
@@ -537,6 +545,7 @@ def test_validate_path_is_safe_bad(path):
         validate_path_is_safe(path)
 
 
+@pytest.mark.skip
 def test_path_validation(mlflow_client):
     experiment_id = mlflow_client.create_experiment("tags validation")
     created_run = mlflow_client.create_run(experiment_id)
@@ -618,12 +627,12 @@ def test_delete_tag(mlflow_client):
     mlflow_client.delete_tag(run_id, "taggity")
     run = mlflow_client.get_run(run_id)
     assert "taggity" not in run.data.tags
-    with pytest.raises(MlflowException, match=r"Run .+ not found"):
+    with pytest.raises(MlflowException, match="Unable to find active run 'fake_run_id'"):
         mlflow_client.delete_tag("fake_run_id", "taggity")
-    with pytest.raises(MlflowException, match="No tag with name: fakeTag"):
+    with pytest.raises(MlflowException, match="Unable to find tag 'fakeTag'"):
         mlflow_client.delete_tag(run_id, "fakeTag")
     mlflow_client.delete_run(run_id)
-    with pytest.raises(MlflowException, match=f"The run {run_id} must be in"):
+    with pytest.raises(MlflowException, match=f"Unable to find active run '{run_id}'"):
         mlflow_client.delete_tag(run_id, "taggity")
 
 
@@ -670,10 +679,11 @@ def test_log_batch_validation(mlflow_client):
                 "run_id": run_id,
                 request_parameter: "foo",
             },
-            f"Invalid value foo for parameter '{request_parameter}' supplied",
+            f"Invalid value for parameter '{request_parameter}' supplied",
         )
 
 
+@pytest.mark.skip
 @pytest.mark.allow_infer_pip_requirements_fallback
 def test_log_model(mlflow_client):
     experiment_id = mlflow_client.create_experiment("Log models")
@@ -727,6 +737,7 @@ def test_set_terminated_status(mlflow_client):
     assert mlflow_client.get_run(run_id).info.end_time <= get_current_time_millis()
 
 
+@pytest.mark.skip
 def test_artifacts(mlflow_client, tmp_path):
     experiment_id = mlflow_client.create_experiment("Art In Fact")
     experiment_info = mlflow_client.get_experiment(experiment_id)
@@ -780,7 +791,7 @@ def test_search_pagination(mlflow_client):
 def test_search_validation(mlflow_client):
     experiment_id = mlflow_client.create_experiment("search_validation")
     with pytest.raises(
-        MlflowException, match=r"Invalid value 123456789 for parameter 'max_results' supplied"
+        MlflowException, match=r"Invalid value for parameter 'max_results' supplied"
     ):
         mlflow_client.search_runs([experiment_id], max_results=123456789)
 
@@ -1044,6 +1055,7 @@ def test_get_metric_history_bulk_calls_optimized_impl_when_expected(monkeypatch,
         )
 
 
+@pytest.mark.skip
 def test_create_model_version_with_local_source(mlflow_client):
     name = "mode"
     mlflow_client.create_registered_model(name)
@@ -1124,6 +1136,7 @@ def test_create_model_version_with_local_source(mlflow_client):
     assert "Invalid source" in resp["message"]
 
 
+@pytest.mark.skip
 def test_logging_model_with_local_artifact_uri(mlflow_client):
     from sklearn.linear_model import LogisticRegression
 
