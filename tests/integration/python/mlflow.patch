diff --git a/pytest.ini b/pytest.ini
index 90cb8c3b8..4506a194f 100644
--- a/pytest.ini
+++ b/pytest.ini
@@ -4,4 +4,3 @@ filterwarnings =
   # Prevent deprecated numpy type aliases from being used
   error:^`np\.[a-z]+` is a deprecated alias for.+:DeprecationWarning:mlflow
   error:^`np\.[a-z]+` is a deprecated alias for.+:DeprecationWarning:tests
-timeout = 1200
diff --git a/tests/tracking/integration_test_utils.py b/tests/tracking/integration_test_utils.py
index f64ea82d1..461a63a2a 100644
--- a/tests/tracking/integration_test_utils.py
+++ b/tests/tracking/integration_test_utils.py
@@ -42,21 +42,16 @@ def _init_server(backend_uri, root_artifact_uri, extra_env=None, app="mlflow.ser
     server_port = get_safe_port()
     with Popen(
         [
-            sys.executable,
-            "-m",
-            "flask",
-            "--app",
-            app,
-            "run",
-            "--host",
-            LOCALHOST,
-            "--port",
-            str(server_port),
+            "fml",
+            "server",
         ],
         env={
             **os.environ,
-            BACKEND_STORE_URI_ENV_VAR: backend_uri,
-            ARTIFACT_ROOT_ENV_VAR: root_artifact_uri,
+            "FML_LISTEN_ADDRESS": f"{LOCALHOST}:{server_port}",
+            "FML_DATABASE_URI": backend_uri,
+            "FML_DEFAULT_ARTIFACT_ROOT": root_artifact_uri,
+            "FML_LOG_LEVEL": "debug",
+            "FML_DATABASE_RESET": str(backend_uri.startswith("postgres://")),
             **(extra_env or {}),
         },
     ) as proc:
diff --git a/tests/tracking/test_rest_tracking.py b/tests/tracking/test_rest_tracking.py
index 5f952dda4..7679a1d7c 100644
--- a/tests/tracking/test_rest_tracking.py
+++ b/tests/tracking/test_rest_tracking.py
@@ -60,7 +60,7 @@ from tests.tracking.integration_test_utils import (
 _logger = logging.getLogger(__name__)
 
 
-@pytest.fixture(params=["file", "sqlalchemy"])
+@pytest.fixture(params=["sqlite", "sqlcipher", "postgres"])
 def mlflow_client(request, tmp_path):
     """Provides an MLflow Tracking API client pointed at the local tracking server."""
     if request.param == "file":
@@ -70,6 +70,12 @@ def mlflow_client(request, tmp_path):
         backend_uri = ("sqlite://" if sys.platform == "win32" else "sqlite:////") + path[
             len("file://") :
         ]
+    elif request.param == "sqlite":
+        backend_uri = f"sqlite://{tmp_path.joinpath('mlflow.db')}"
+    elif request.param == "sqlcipher":
+        backend_uri = f"sqlite://{tmp_path.joinpath('mlflow.db')}?_key=passphrase"
+    elif request.param == "postgres":
+        backend_uri = "postgres://postgres:postgres@postgres/postgres"
 
     with _init_server(backend_uri, root_artifact_uri=tmp_path.as_uri()) as url:
         yield MlflowClient(url)
@@ -146,7 +152,7 @@ def test_create_experiment_validation(mlflow_client):
         {
             "name": 123,
         },
-        "Invalid value 123 for parameter 'name'",
+        "Invalid value for parameter 'name'",
     )
     assert_bad_request({}, "Missing value for required parameter 'name'")
     assert_bad_request(
@@ -155,7 +161,7 @@ def test_create_experiment_validation(mlflow_client):
             "artifact_location": 9.0,
             "tags": [{"key": "key", "value": "value"}],
         },
-        "Invalid value 9.0 for parameter 'artifact_location'",
+        "Invalid value for parameter 'artifact_location'",
     )
     assert_bad_request(
         {
@@ -163,7 +169,7 @@ def test_create_experiment_validation(mlflow_client):
             "artifact_location": "my_location",
             "tags": "5",
         },
-        "Invalid value 5 for parameter 'tags'",
+        "Invalid value for parameter 'tags'",
     )
 
 
@@ -328,7 +334,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": 59,
             "step": 26,
         },
-        "Invalid value 31 for parameter 'run_id' supplied",
+        "Invalid value for parameter 'run_id' supplied",
     )
     assert_bad_request(
         {
@@ -338,7 +344,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": 59,
             "step": 26,
         },
-        "Invalid value 31 for parameter 'key' supplied",
+        "Invalid value for parameter 'key' supplied",
     )
     assert_bad_request(
         {
@@ -348,7 +354,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": 59,
             "step": "foo",
         },
-        "Invalid value foo for parameter 'step' supplied",
+        "Invalid value for parameter 'step' supplied",
     )
     assert_bad_request(
         {
@@ -358,7 +364,7 @@ def test_log_metric_validation(mlflow_client):
             "timestamp": "foo",
             "step": 41,
         },
-        "Invalid value foo for parameter 'timestamp' supplied",
+        "Invalid value for parameter 'timestamp' supplied",
     )
     assert_bad_request(
         {
@@ -412,7 +418,7 @@ def test_log_param_validation(mlflow_client):
             "key": "param",
             "value": 41,
         },
-        "Invalid value 31 for parameter 'run_id' supplied",
+        "Invalid value for parameter 'run_id' supplied",
     )
     assert_bad_request(
         {
@@ -420,7 +426,7 @@ def test_log_param_validation(mlflow_client):
             "key": 31,
             "value": 41,
         },
-        "Invalid value 31 for parameter 'key' supplied",
+        "Invalid value for parameter 'key' supplied",
     )
 
 
@@ -482,7 +488,7 @@ def test_set_tag_validation(mlflow_client):
             "key": "tag",
             "value": 41,
         },
-        "Invalid value 31 for parameter 'run_id' supplied",
+        "Invalid value for parameter 'run_id' supplied",
     )
     assert_bad_request(
         {
@@ -490,7 +496,7 @@ def test_set_tag_validation(mlflow_client):
             "key": "param",
             "value": 41,
         },
-        "Invalid value 41 for parameter 'value' supplied",
+        "Invalid value for parameter 'value' supplied",
     )
     assert_bad_request(
         {
@@ -533,17 +539,11 @@ def test_path_validation(mlflow_client):
     assert_response(response)
 
     response = requests.get(
-        f"{mlflow_client.tracking_uri}/get-artifact",
+        f"{mlflow_client.tracking_uri}/ajax-api/2.0/mlflow/artifacts/get",
         params={"run_id": run_id, "path": invalid_path},
     )
     assert_response(response)
 
-    response = requests.get(
-        f"{mlflow_client.tracking_uri}//model-versions/get-artifact",
-        params={"name": "model", "version": 1, "path": invalid_path},
-    )
-    assert_response(response)
-
 
 def test_set_experiment_tag(mlflow_client):
     experiment_id = mlflow_client.create_experiment("SetExperimentTagTest")
@@ -602,7 +602,7 @@ def test_delete_tag(mlflow_client):
     with pytest.raises(MlflowException, match="No tag with name: fakeTag"):
         mlflow_client.delete_tag(run_id, "fakeTag")
     mlflow_client.delete_run(run_id)
-    with pytest.raises(MlflowException, match=f"The run {run_id} must be in"):
+    with pytest.raises(MlflowException, match=f"Run '{run_id}' not found"):
         mlflow_client.delete_tag(run_id, "taggity")
 
 
@@ -649,13 +649,13 @@ def test_log_batch_validation(mlflow_client):
                 "run_id": run_id,
                 request_parameter: "foo",
             },
-            f"Invalid value foo for parameter '{request_parameter}' supplied",
+            f"Invalid value for parameter '{request_parameter}' supplied",
         )
 
     ## Should 400 if missing timestamp
     assert_bad_request(
         {"run_id": run_id, "metrics": [{"key": "mae", "value": 2.5}]},
-        "Invalid value [{'key': 'mae', 'value': 2.5}] for parameter 'metrics' supplied",
+        "Invalid value for parameter 'metrics' supplied",
     )
 
     ## Should 200 if timestamp provided but step is not
@@ -668,7 +668,7 @@ def test_log_batch_validation(mlflow_client):
     assert response.status_code == 200
 
 
-@pytest.mark.allow_infer_pip_requirements_fallback
+@pytest.mark.skip()
 def test_log_model(mlflow_client):
     experiment_id = mlflow_client.create_experiment("Log models")
     with TempDir(chdr=True):
@@ -778,7 +778,7 @@ def test_search_pagination(mlflow_client):
 def test_search_validation(mlflow_client):
     experiment_id = mlflow_client.create_experiment("search_validation")
     with pytest.raises(
-        MlflowException, match=r"Invalid value 123456789 for parameter 'max_results' supplied"
+        MlflowException, match=r"Invalid value for parameter 'max_results' supplied"
     ):
         mlflow_client.search_runs([experiment_id], max_results=123456789)
 
@@ -1003,6 +1003,7 @@ def test_get_metric_history_bulk_respects_max_results(mlflow_client):
     ]
 
 
+@pytest.mark.skip
 def test_get_metric_history_bulk_calls_optimized_impl_when_expected(tmp_path):
     from mlflow.server.handlers import get_metric_history_bulk_handler
 
@@ -1045,6 +1046,7 @@ def test_get_metric_history_bulk_calls_optimized_impl_when_expected(tmp_path):
         )
 
 
+@pytest.mark.skip()
 def test_search_dataset_handler_rejects_invalid_requests(mlflow_client):
     def assert_response(resp, message_part):
         assert resp.status_code == 400
@@ -1080,6 +1082,7 @@ def test_search_dataset_handler_rejects_invalid_requests(mlflow_client):
     )
 
 
+@pytest.mark.skip()
 def test_search_dataset_handler_returns_expected_results(mlflow_client):
     experiment_id = mlflow_client.create_experiment("log inputs test")
     created_run = mlflow_client.create_run(experiment_id)
@@ -1113,6 +1116,7 @@ def test_search_dataset_handler_returns_expected_results(mlflow_client):
     assert response.json().get("dataset_summaries") == [expected]
 
 
+@pytest.mark.skip()
 def test_create_model_version_with_path_source(mlflow_client):
     name = "model"
     mlflow_client.create_registered_model(name)
@@ -1153,6 +1157,7 @@ def test_create_model_version_with_path_source(mlflow_client):
     assert "To use a local path as a model version" in response.json()["message"]
 
 
+@pytest.mark.skip()
 def test_create_model_version_with_non_local_source(mlflow_client):
     name = "model"
     mlflow_client.create_registered_model(name)
@@ -1313,6 +1318,7 @@ def test_create_model_version_with_non_local_source(mlflow_client):
     assert "If supplying a source as an http, https," in response.json()["message"]
 
 
+@pytest.mark.skip()
 def test_create_model_version_with_file_uri(mlflow_client):
     name = "test"
     mlflow_client.create_registered_model(name)
@@ -1393,6 +1399,7 @@ def test_create_model_version_with_file_uri(mlflow_client):
     assert "MLflow tracking server doesn't allow" in response.json()["message"]
 
 
+@pytest.mark.skip()
 def test_create_model_version_with_file_uri_env_var(tmp_path):
     backend_uri = tmp_path.joinpath("file").as_uri()
     with _init_server(
@@ -1417,6 +1424,7 @@ def test_create_model_version_with_file_uri_env_var(tmp_path):
         assert response.status_code == 200
 
 
+@pytest.mark.skip()
 def test_logging_model_with_local_artifact_uri(mlflow_client):
     from sklearn.linear_model import LogisticRegression
 
@@ -1427,6 +1435,7 @@ def test_logging_model_with_local_artifact_uri(mlflow_client):
         mlflow.pyfunc.load_model("models:/rmn/1")
 
 
+@pytest.mark.skip()
 def test_log_input(mlflow_client, tmp_path):
     df = pd.DataFrame([[1, 2, 3], [1, 2, 3]], columns=["a", "b", "c"])
     path = tmp_path / "temp.csv"
@@ -1461,6 +1470,7 @@ def test_log_input(mlflow_client, tmp_path):
     assert dataset_inputs[0].tags[1].value == "train"
 
 
+@pytest.mark.skip()
 def test_log_inputs(mlflow_client):
     experiment_id = mlflow_client.create_experiment("log inputs test")
     created_run = mlflow_client.create_run(experiment_id)
@@ -1490,6 +1500,7 @@ def test_log_inputs(mlflow_client):
     assert run.inputs.dataset_inputs[0].tags[0].value == "value1"
 
 
+@pytest.mark.skip()
 def test_log_inputs_validation(mlflow_client):
     experiment_id = mlflow_client.create_experiment("log inputs validation")
     created_run = mlflow_client.create_run(experiment_id)
@@ -1537,6 +1548,7 @@ def test_update_run_name_without_changing_status(mlflow_client):
     assert updated_run_info.status == "FINISHED"
 
 
+@pytest.mark.skip()
 def test_create_promptlab_run_handler_rejects_invalid_requests(mlflow_client):
     def assert_response(resp, message_part):
         assert resp.status_code == 400
@@ -1626,6 +1638,7 @@ def test_create_promptlab_run_handler_rejects_invalid_requests(mlflow_client):
     )
 
 
+@pytest.mark.skip()
 def test_create_promptlab_run_handler_returns_expected_results(mlflow_client):
     experiment_id = mlflow_client.create_experiment("log inputs test")
 
@@ -1669,6 +1682,7 @@ def test_create_promptlab_run_handler_returns_expected_results(mlflow_client):
     ]["tags"]
 
 
+@pytest.mark.skip()
 def test_gateway_proxy_handler_rejects_invalid_requests(mlflow_client):
     def assert_response(resp, message_part):
         assert resp.status_code == 400
@@ -1693,6 +1707,7 @@ def test_gateway_proxy_handler_rejects_invalid_requests(mlflow_client):
         )
 
 
+@pytest.mark.skip()
 def test_upload_artifact_handler_rejects_invalid_requests(mlflow_client):
     def assert_response(resp, message_part):
         assert resp.status_code == 400
@@ -1738,6 +1753,7 @@ def test_upload_artifact_handler_rejects_invalid_requests(mlflow_client):
     assert_response(response, "Request must specify data.")
 
 
+@pytest.mark.skip()
 def test_upload_artifact_handler(mlflow_client):
     experiment_id = mlflow_client.create_experiment("upload_artifacts_test")
     created_run = mlflow_client.create_run(experiment_id)
